{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "To625fCJDk4v",
    "outputId": "fd4835fa-b717-4470-faea-e0ea30aa59bf"
   },
   "source": [
    "Importing all the libraries for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from numpy import loadtxt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "from sklearn.utils import shuffle\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix a random seed for both TensorFlow and Numpy\n",
    "seed = 2\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the training set to train the first neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxs1wgOXDlwy",
    "outputId": "86b840bd-01e5-4b64-eea6-80f1ccb8b904"
   },
   "outputs": [],
   "source": [
    "tr = loadtxt(\"dataset/monk1/monks-1.train\", dtype='str', delimiter=' ')\n",
    "# The first column and the last of this matrix don't contain any useful information for our project,\n",
    "# so we can delete them\n",
    "tr = np.delete(tr, [0, 8], 1) \n",
    "# Now the first column of the matrix contains the target, while the other six the input for the NN. Let us now \n",
    "# convert to integers all the values of the matrix\n",
    "tr = tr.astype(int)\n",
    "tr[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to divide the column of the matrix into target and input. In this case the target is the first column of the matrix, while the input are the other 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSoWTIrqB35x"
   },
   "outputs": [],
   "source": [
    "# target\n",
    "y = tr[:, 0]\n",
    "# input\n",
    "x = tr[:, 1:7]\n",
    "\n",
    "x, y = shuffle(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "one_hot_encoder.fit(x)\n",
    "x = one_hot_encoder.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to implement early stopping to understand where we have to stop the training of our neural network, for this reason we need to split our dataset to distinguish between the training set and the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(\"size of the design set\", len(x))\n",
    "\n",
    "# we are going to use 30% of our training set as the validation set\n",
    "subset_size = int(0.3 * len(x))\n",
    "\n",
    "# find randomly the indexes of the rows for the validation set\n",
    "index = np.random.choice(len(x), subset_size, replace=False)\n",
    "index = np.sort(index)\n",
    "\n",
    "# create the validation set\n",
    "vl_x = x[index]\n",
    "vl_y = y[index]\n",
    "\n",
    "# create the training set\n",
    "tr_x = np.delete(x, index, 0)\n",
    "tr_y = np.delete(y, index, 0)\n",
    "\n",
    "print(\"size of the training set\", len(tr_x))\n",
    "print(\"size of the validation set\", len(vl_y))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZUNdNC8CIZx",
    "outputId": "5bc4bbe9-20e9-4c18-ea32-f5130debd002"
   },
   "source": [
    "So we are now ready to build the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a model, it is required to perform a grid search over the hyper-parameters\n",
    "def create_model():\n",
    "    # (From Keras documentation) A Sequential model is appropriate for a plain stack of layers where each layer \n",
    "    # has exactly one input tensor and one output tensor. \n",
    "    # In other words, in a sequential model, the input flows sequentially from the first layers to the last one,\n",
    "    # without self-loops or backward edges\n",
    "    model = Sequential();\n",
    "    # The dense class is used to implement a densely connected neural network, which is a neural network where each \n",
    "    # input is connected to every output by a weight.\n",
    "    # The parameter \"input_space\" specify the size of the input space (in this case 6, since there are exactly\n",
    "    # 6 columns in tr_x). This parameter must be added to the first hidden layer of the NN.\n",
    "    model.add(Dense(4, input_dim=17, activation=\"relu\"))\n",
    "    # We have added a hidden layer to our neural network (which also works as an input layer, since it fetches the\n",
    "    # input), now let's add similarly an output layer.\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # Now we can compile the neural network. To compile it we need to specify: the loss function (which is\n",
    "    # binary_crossentropy for binary classificatio), the optimizer (in this case we'll use SGD, which is the \n",
    "    # stochastic gradient descent algorithm), the metrics to collect and report the performance of the\n",
    "    # neural network (in this case we will use the accuracy, since it is a classification problem).\n",
    "    model.compile(loss='mean_squared_error', optimizer=\"SGD\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class from the module SciKeras. We will use the KerasClassifier since we have to solve a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter verbose = 0 is used to silence the output.\n",
    "model = KerasClassifier(model=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the values and the hyperparameters for the grid search. Below there is a list of hyperparameters that we'll consider for the grid search.\n",
    "\n",
    "1. The learning rate for the gradient descent algorithm\n",
    "2. the value of alpha for momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = [20, 60, 100]\n",
    "epochs = [1000]\n",
    "learn_rate = [0.01, 0.1, 0.2]\n",
    "momentum = [0.0, 0.1, 0.2, 0.3]\n",
    "\n",
    "# In the SciKeras wrapper, to route the parameters to the optimizer we need the prefix optimizer__.\n",
    "param_grid = dict(\n",
    "    optimizer__learning_rate=learn_rate,\n",
    "    optimizer__momentum=momentum, \n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV is used to perform an exhaustive search over specified parameter values for an estimator. We will use\n",
    "# it to find the best set of hyper-parameters for the model selection of our neural network.\n",
    "# param_grid = Dictionary with parameters names (str) as keys and lists of parameter settings to try as values.\n",
    "# n_jobs = Number of jobs to run in parallel, the value -1 will use all the available processors.\n",
    "# cv = number of folds of the cross validation.\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, error_score=\"raise\")\n",
    "\n",
    "# a simple condition for early stopping. Patience stops the training only if the loss on the validation set rose\n",
    "# for a number of consecutive epochs \n",
    "es = EarlyStopping(monitor='loss', mode='min', patience=10)\n",
    "\n",
    "'''\n",
    "grid_result = grid.fit(\n",
    "    tr_x, tr_y, # training set\n",
    "    validation_data=(vl_x, vl_y), # validation set\n",
    "    verbose=1, # to print the results\n",
    "    shuffle=True, # shuffle before each epoch\n",
    "    callbacks=[es], # callbacks to be executed at the end of each epoch\n",
    "    epochs = 500 # ELIMINA POI\n",
    ")'''\n",
    "\n",
    "'''\n",
    "grid_result = grid.fit(\n",
    "    x, y, # training set\n",
    "    validation_split=0.3, # validation set's dimension\n",
    "    verbose=1, # to print the results\n",
    "    shuffle=True, # shuffle before each epoch\n",
    "    callbacks=[es], # callbacks to be executed at the end of each epoch\n",
    "    epochs = 100 # ELIMINA POI\n",
    ")'''\n",
    "\n",
    "grid_result = grid.fit(\n",
    "    x, y,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the best configuration of hyper-parameters found by the grid search and its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LE CASELLE SOTTO A QUESTA SONO SOLO DELLE PROVE PER IL CODICE E ANDRANNO ELIMINATE NELLA VERSIONE FINALE DEL PROGETTO #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# QUESTA CELLA Ãˆ PER UN TEST, ELIMINARE POI\n",
    "\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=6, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=50)#, min_delta=1, verbose=1)\n",
    "# fit model\n",
    "history = model.fit(tr_x, tr_y, validation_data=(vl_x, vl_y), epochs=4000, verbose=0, callbacks=[es])\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(tr_x, tr_y, verbose=0)\n",
    "_, test_acc = model.evaluate(vl_x, vl_y, verbose=0)\n",
    "print('Train: %.3f, Validation: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyplot.plot(history.history['accuracy'], label='train')\n",
    "pyplot.plot(history.history['val_accuracy'], label='validation')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, accuracy = model.evaluate(tr_x, tr_y) # verbose=0 to suppress output\n",
    "print('Accuracy: %.2f' % (accuracy*100)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALTRO ESPERIMENTO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(3, input_dim=17, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "\n",
    "history2 = model2.fit(x, y, epochs=1000, batch_size=20, verbose=1)\n",
    "# evaluate the model\n",
    "_, train_acc = model2.evaluate(x, y, verbose=0)\n",
    "print('Train: %.3f' % train_acc)\n",
    "\n",
    "tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01, momentum=0.2, nesterov=False, name=\"SGD\"\n",
    ")\n",
    "\n",
    "# plot training history\n",
    "pyplot.plot(history2.history['loss'], label='train')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(history2.history['accuracy'], label='train')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = history2.history['accuracy'][999]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
